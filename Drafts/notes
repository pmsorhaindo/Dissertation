Questions
Always talk about the project in past tense?
Design section contain, intended design and final design?
ie should I talk about modifications to the design during implementation?




Intro
Choice
Modules used
Why

Initial ideas and decisions

Methodology
How I plan to do it
How to research
How to code

Background research and the way its influenced the project

Coding section
Problems
Solutions Lessons learnt

Further Work

Conclusion


Intro Notes

Strategic parallelism
There are two basic strategies for deciding how to partition a program. With implicit partitioning, the compilation system decides which tasks should be created; with explicit partitioning, however, the programmer is left with the problem of determining which expressions should be created as tasks. In either case, the partition could be static, in which case the number of tasks which will be created at runtime is predetermined, or dynamic, in which case tasks are created depending on factors such as the overall runtime load, or load control annotations. Tasks may be placed on the processor creating the task, on the processor owning the data which the task requires, or on some other processor. Task placement may also be explicit or implicit, static or dynamic.

It is, of course, vitally important to choose tasks of an appropriate granularity (or duration) for the target machine. The best partition for a given machine will be one which maximises the available parallelism, up to the number of processors available, while minimising the parallel overhead. Coarse-grained tasks are desirable in order to minimise task creation overheads. However, excessively coarse granularity can lead to increased idle time if too few tasks are created, and can also introduce high task migration overheads, if the load is imperfectly distributed (Section 4.1).

nfib n = if n <= 1 then 1 
         else 1 + nfib(n-1) + nfib(n-2)


Concurrency is needed in languages, but the aim is to make programs run faster on a multicore, [Concurrency should be a last resort].

This is regardless of what synchronisation technology you’re using: yes, STM is better than locks, and message passing has its advantages, but All of these are just ways to communicate between threads in a non-deterministic language.


In contrast, a parallel program solves a single problem. Consider a financial model that attempts to predict the next minute of fluctuations in the price of a single stock. If we want to apply this model to every stock listed on an exchange, for example to estimate which ones we should buy and sell, we hope to get an answer more quickly if we run the model on five hundred cores than if we use just one. As this suggests, a parallel program does not usually depend on the presence of multiple cores to work correctly.
Another useful distinction between concurrent and parallel programs lies in their interaction with the outside world. By definition, a concurrent program deals continuously with networking protocols, databases, and the like. A typical parallel program is likely to be more focused: it streams data in, crunches it for a while (with little further I/O), then streams data back out.

Many traditional languages further blur the already indistinct boundary between concurrent and parallel programming, because they force programmers to use the same primitives to construct both kinds of program.
In this chapter, we will concern ourselves with concurrent and parallel programs that operate within the boundaries of a single operating system process.

So where did this dangerous assumption that Parallelism == Concurrency come from?  
It’s a natural consequence of languages with side-effects: when your language has side-effects everywhere, then any time you try to do more than one thing at a time you essentially have non-determinism caused by the interleaving of the effects from each operation.  So in side-effecty languages, the only way to get parallelism is concurrency; it’s therefore not surprising that we often see the two conflated.

However, in a without side affects, it is possible to run different parts of a program at the same time without any differences occuring in its result.  This is one reason that our salvation lies in programming languages with controlled side-effects. xxxxxThe way forward for those side-effecty languages is to start being more explicit about the effects, so that the effect-free parts can be identified and exploited.xxxxxxxx

It pains me to see Haskell’s concurrency compared against the concurrency support in other languages, when the goal is simply to make use of multicore CPUs (Edit: Ted followed up with a clarification).   It’s missing the point: yes of course Haskell has the best concurrency support  , but for this problem domain it has something even better: deterministic parallelism.  In Haskell you can use multicore CPUs without getting your hands dirty with concurrency and non-determinism, without having to get the synchronisation right, and with a guarantee that the parallel program gives the same answer every time, just more quickly.

There are two facets to Haskell’s determinstic parallelism support:
par/pseq and Strategies. These give you a way to add parallelism to an existing program, usually without requiring much restructuring.  For instance, there’s a parallel version of ‘map’. Support for this kind of parallelism is maturing with the soon to be released GHC 6.12.1, where we made some significant performance improvements over previous versions.

Nested Data Parallelism.  This is for taking advantage of parallelism in algorithms that are best expressed by composing operations on (possibly nested) arrays.  The compiler takes care of flattening the array structure, fusing array operations, and dividing the work amongst the available CPUs.  Data-Parallel Haskell will let us take advantage of GPUs and many-core machines for large-scale data-parallelism in the future.  Right now, DPH support in GHC is experimental, but work on it continues.

That’s not to say that concurrency doesn’t have its place.  So when should you use concurrency?  Concurrency is most useful as a method for structuring a program that needs to communicate with multiple external clients simultaneously, or respond to multiple asynchronous inputs.  It’s perfect for a GUI that needs to respond to user input while talking to a database and updating the display at the same time, for a network application that talks to multiple clients simultaneously, or a program that communicates with multiple hardware devices, for example.  Concurrency lets you structure the program as if each individual communication is a sequential task, or a thread, and in these kinds of settings it’s often the ideal abstraction.  STM is vitally important for making this kind of programming more tractable.

As luck would have it, we can run concurrent programs in parallel without changing their semantics.  However, concurrent programs are often not compute-bound, so there’s not a great deal to be gained by actually running them in parallel, except perhaps for lower latency.

However, there is some overlap between concurrency and parallelism.
Some algorithms use multiple threads for parallelism deliberately; for example, search-type problems in which multiple threads search branches of a problem space, where knowledge gained in one branch may be exploited in other concurrent searches.  SAT-solvers and game-playing algorithms are good examples.
An open problem is how to incorporate this kind of non-deterministic parallelism in a safe way: in Haskell these algorithms would end up in the IO monad, despite the fact that the result could be deterministic.
Still, I believe these kinds of problems are in the minority, and we can get a long way with purely deterministic parallelism.

With GHC it is possible to mix parallelism and concurrency on multicore CPUs at will.

In order to explore the effects of parallelism this [project] will take on the problem of simulating an Ant colony. Ant colonies are a large problem, in perspective, often containing tens of thousands of ants within it's [ecosystem]. Each ant is a seperate entity acting of its own free will, yet the colony works together towards a common purpose. Due to the fact each ant is it's own entity it is capabable of making decisions at exactly the same time as other ants. To model this effectively would require the ant's decision making to happen in parallel.




Haskell can use structured programming processes, except haskell has the ability to further abstract using Monads

<Philippa> we also use a lot of EDSLs though, which can be good for modelling

<_Mikey> Philippa, ok, do you mean using Type signatures to lay out a structure?
<Philippa> and what do you want to be able to do with your model? Just introspect it as source, or something more involved?
<_Mikey> Botje, on the function level would be fine. Philippa, just for documentation of my project really.

<Philippa> _Mikey: really I mean a collection of techniques that were around prior to the big 90s OO craze. Modular design, various flow diagrams etc etc (which will be a partial model of code flow as well, just one with no notion of 'time')

<Philippa> using type signatures and stub functions can also be useful if you're after that kind of model of course
<_Mikey> Yea, I've found that really useful during development

<_Mikey> Philippa, but I was just wondering if there is a standard 'Haskell' way of doing things
<Philippa> there's no single standard really, Haskell's community has a TIMTOWTDI approach to things


<efie> philippa: I'm uncertain when to make own monad instances / how to really do it (though i read some examples) and I'd like to practise this

<Philippa> did I miss anything?
<Philippa> efie: fair enough. I don't have a good set of exercises to hand. Personally I recommend building a new instance the moment you want something that's not covered by existing instances but should make an okay "haskell-like language" (definition to be supplied) and covers how you want to express your code. Mileage varies on that. I also recommend building instances from existing instances where
 possible! Wrapping existing monads or transformer stacks is pretty effective
<Philippa> writing an interpreter for a language and bolting on features is a good way to explore the monad transformers from the libs, at least
 feel free to skip writing a parser in favour of just building AST nodes in Haskell!

<Philippa> if you write yourself a typechecker that does inference for the simply-typed lambda calculus or Hindley-Milner you'll quickly find yourself wanting a monad to pipe around your typing problem too
<Philippa> transformers're actually easier than writing your own instance from scratch
<Philippa> Botje: or at least, not yet I assume


<_Mikey> is there a specific chapter in RWH which focuses on the design of programs? other than the 

<Philippa> _Mikey: aside from having to build 'dataflow piping' of various sorts, you can treat it just like old-school procedural programming and it'll work out

<Philippa> efie: does it make sense when I say "write an interpreter"? (to implement a new monad)
<hiptobecubic> Philippa, it doesn't really make sense to me.
<Philippa> hiptobecubic: okay, guess I get to write the identity monad as a deep embedding yet again :-)
 data Identity a where Return :: a -> Identity a; Bind :: Identity a -> (a -> Identity b) -> Identity b
<hiptobecubic> Philippa, i'm still learning about monads and categories. Had a nice long talk last night with DrSyzygy about it :)


<Philippa> there's the syntax for the lang corresponding to the identity monad
<Philippa> _Mikey: one good way is as lots of programs in DSLs, interacting via thin layers of glue code. Think *nix shell scripting, in Haskell
<Axman6> _Mikey: many people tend to start with the data types they're going to need. then implement the functions for working with them, then writing modules that bring it all together. how you do that will probably be different for each project however

<Philippa> hiptobecubic: when you come to write your class instance, return = Return, (>>=) = Bind and you have to hide Identity's constructors to avoid breaching the monad laws. Need to see the interpreter as well?

<Jafet> I've seen many different ways to model Haskell programs, There is nothing you can look at that will instantly convey how to design software

<Philippa> Jafet: yeah, there's a lot and most have their uses. That's one reason there aren't many "just do this" recommendations
<osfameron> probably most of the advice in, say, Code Complete, applies as much to FP as to imperative/OO programming
<osfameron> it's all about how you decompose the problem
<Philippa> having pervasive, cheap parametric polymorphism does change things a little though
<_Mikey> Philippa, I like the DSL idea,

<Philippa> but I think the biggest piece of advice I'd offer is to try to write code that's high level in the Perlis sense  (plus the code needed to enable that)



<merijn> Data Parallel Haskell(screencast)
<osfameron> Philippa: what do you mean about "pervasive, cheap parametric polymorphism" changing things?
<Philippa> osfameron: the stuff you get from having an H-M-derived type system rather than, say, STLC
<Philippa> we don't even think about having it most of the time, because the system just figures out where we've got it
 but once we know it's there, we can use it to make sure we don't overconstrain ourselves

<osfameron> Philippa: sure.  but does that change the *way* you build programs?
<merijn> osfameron: Compared to say Java/C? Sure.
<Philippa> massively, unless your starting point is dynamic typing where you can pretend you've got the same and hope nobody breaks it
<merijn> Much easier to define nice generic combinators to combine your code, which you cannot do in C
<osfameron> Philippa: ah, gotcha.  cos I mostly write in dynamic typed languages, it's very much the same process

<osfameron> again though, I'm not sure it's that different in process from writing Java/C in terms of design of code, it's just faster

<Philippa> hah. No, people do dirty shit in langs without parametric polymorphism, or they don't push it as far if it's not cheap and pervasive
<Philippa> there are subcultures around C that act like they've got it to some extent, but you don't see the same kind of stuff that you do in Haskell there
 Philippa: You just hack it in with CPP :D
<_Mikey> whats CPP?
<merijn> _Mikey: C Pre-Processor
<merijn> i.e. C macro's
