\documentclass{report}

\begin{document}

\title {Planning and Research Report}
\author {Paul-Michael Sorhaindo}
\date {October 2011}
\maketitle
\begin{abstract}
%(!TODO) Abstract
Abstract goes here..
\end{abstract}


\chapter{Introduction}
\section{About this paper}
In this paper an outline for the project will be put forward, research will be carried out and reported on, a plan will be proposed as to how to carry out the rest of the project with small targets and goals and finally areas of further research will be identified. The paper will be divided into four significant chapters the introduction, in which background will be provided as to how the topic for the project was chosen, relevance to the course and the project's objectives and aims. This will be followed by a research chapter which begins by outlining research aims and continues to report on my initial research into some of these aims. The chapter following this will consist of aims and goals in a chronological order that will be viewed as form of schedule for the project. Finally the report will conclude with a discussion about the areas that are as of yet not fully researched but will be read into further as the project progresses.

\section{Producing a Topic}
The process of settling on a topic for a project took quite some time the initial idea was to produce something which was in the Natural Language Processing field. The primary aim was to implement an intelligent agent that would parse text, recognizing relationships between pronouns and the nouns they refer to. Then storing everything in a database that it parsed so that it could analyze the data statistically and self optimize the parsing rules I initially gave it in order to parse more efficiently and accurately. Furthermore I wanted to extend that idea further and include text generation functionality to the proposed project and allow it to communicate with a user via a keyboard and screen so it would effectively become a chat-bot which ``learnt'' from its mistakes. After further research into the NLP field I realized how ambitious these plans were and that just the subject of data mining a corpus of descriptive text in isolation is a broad field. I also interested exploring the field of AI in particular genetic algorithms. The main concept of the idea was to recreate a race track and racing car and implement an intelligent agent with a limited view of the track such as a human would have and after exposing functionality of the car such as a human would have give the agent the goal of completing the track in the least amount of time possible. It turned out I didn't go with this idea because it would have been an incredible amount of work to develop the track and car and to accurately simulate all the physics involved that it was possible that the AI element wouldn't get time required to produce a finished artifact.

\paragraph{}After reading around the field of AI I came across the topic of swarm intelligence, which was first used in reference to robots. The definition given by Beni, G. and Wang, J. (1989) (!cite) of Swarm Intelligence is ``systems of non-intelligent robots exhibiting collectively intelligent behaviour evident in the ability to unpredictably produce `specific' ([i.e.] not in a statistical sense) ordered patterns of matter in the external environment'' the idea of implementing a system that could simulate a group of simple objects that together worked intelligently appealed to me. I looked to the real world for something to use as a model, so I could simulate on a computer with AI, and I realized that ants and an ant colony as a whole would offer a great model to simulate after analyzing the problem and how it would be implemented (with my supervisor), I realized that again the problem consisted of many issues the first of which was ant colonies often consist of tens of thousands, if not more, ants. Simulating that many objects which are all moving independently each with their own location and each with their own task, has the potential to be very processor intensive. As the number of ants in the simulation increase, depending on the implementation used, the simulation model could easily slow significantly and stop, producing incorrect values no longer in real time resulting in something with high latency and therefore inaccurate. In the context of a casual simulation the accuracy and need for it to be real time aren't imperative but, in the computer games industry this is increasingly becoming not the case. The games produced by the big development houses which really turnover money, in terms of the amount spent developing them and the amount they sell for, come under more and more scrutiny from the average game player as they expect their gaming experience to become more immersive and realistic. Although the realtime in games, often classified as soft realtime because the usefulness of information recieved after it is due degrades the longer the delay is, the market has become such that game development houses who allow their products too push the leaniancy of the soft realtime requirements will suffer at the hand of bad reviews. As the major companies in the industry who produce AAA titles sell alot of releases based of the reputation their previous products have a good reputation, as in most industries, is vital.

\section{Relevance to degree}
Making games has been described by many as an art, (Wired reference) and just as in the art of story telling it is important for characters to have life and feel real so too it's important for many games because of the aesthetic path they've chosen to present every object and every movement within as realistically as the end users machine will allow in order to fully immerse the player. Ofcourse there are many tricks a programmer can employ to fake the feeling of real physics, true artificial intelligence and produce graphics that stun the gamer. But these tricks often get swapped out for the real deal when technology catches up with designers imaginations. Way back games were displayed on monochrome screens and made use of coloured film over the display to create the illusion that different colours were on the screen, while today most people think nothing of their 1080p widescreen displays which absorb our natural field of view with high definition. There is always a demand for accuracy within computer games and research tries to meet these demands. Take for example one of the most accurate forms of rendering in computer graphics, ray tracing, in August of 2009 Nvidia launched what they dubbed 'The World's First Interactive Ray Tracing Engine' the engine OptiX (<?better source) which is just the result of one of many projects which will ultimately allow developers to bring games and many other application of computers to another level of accuracy and realism.

\section {Overall Project Aims and Objectives}
The fundemental aim of this project is to produce a simulation of an ant colony, exploiting the use of parallel programming, which reperents ants as individual units working independently without an overarching, all-knowing method which controls each one. However this aim can be split up into several smaller aims. The aims are detailed below because it is important to highlight different areas under which the project can be assessed. I have split the aims up in under the following headings, Technical, Qualititive and Learning outcomes. It is important that any project has each of these as they are all measured in different ways. Technical aims are absolute while Qualititve aims are subjective. Finally whether or the learning aims have been achieved or not can only be decided by the person conducting the project.

\subsection{Technical Aims}
\begin{itemize}
	\item To explore paralellism and implement parallel and sequential algorithms.
	\item To produce functional code that addresses the problem.
	\item To implement accurate collision detection.
	\item To implement a data structure that holds the information in the simulation.
	\item To develop custom algorithms suitable for this project.
\end{itemize}

\subsection{Qualititive Aims}
\begin{itemize}
	\item To produce ants which have a sense of  position and can navigate their environment
	\item To produce ants which appear to interact through the use of phermones
	\item To produce an environment that provides obstacles that tests the ants behaviour
	\item To produce clear concise and understandable code.
\end{itemize}

\subsection{Learning Outcomes}
\begin{itemize}
	\item To be able to program functionally
	\item To have a better understanding of parallelism
	\item To have a good understanding of the programming language(s) that I use to implement my solution.
	\item To be able to analyze and critque algorithms effectively.
\end{itemize}

\section{Project overview}
In this project the problem of simulating the independant movements of a group of ants is tackled, instead of taking a short-cut approach to the problem and allowing every ant to see the whole 'world' the ants will have a realistic limited field of sense each going about their own tasks independantly but while sharing the common goals of the colony. This will provide several problems which the project will attempt to address over the course of the project there will be a particular focus on the amount ofants working individually yet simultaneously, while trying to keep the nature of their interaction as accurate as possible. However it is important to mention here that when  the terms realistic and accurate are used they will be used with the meaning ``accurately conforming to a list of given assumptions'', as detailed below, because the finer details of how an ant or ant colony functions is not the point of this project. However some brief reading of secondary sources will be done in order to produce a list of assumptions which will define the realistic (to a given definition of realistic) behaviour of an ant and ant colony which hopefully should be agreeable for the average non-entomologist. After this we will start analyzing and discussing various approaches to tackling the problem that has been posed, firstly abstractly then programatically looking at various programming paradigms. Then research will continue further on methods mentioned in this disscussion that seem most likely to provide a solution and various ways of implementing the solution will be presented using the techniques discovered during this initial research period. 

\section{Ant Colony Generalizations}
\paragraph{}Listed here are the aforementioned generalizations or preconceptions about ants and their colonies, that will be assumed as factual for the purposes of this project. All ants need both food and water to survive, according to the Center for Insect Science Education Outreach at the University of Arizona ants can go for quite some time without food but without water they will be dead within a day. For this reason water will be a stronger priority for all ants over food and ants within the simulation will die without water or food, I will determine the amount of time a satiated ant can survive for as three simulation days without food and six simulation hours without water. After reading an article on the BBC News website about sleeping patterns in ants I learnt that that queen ants live for years where as worker ants live for months because of this statement I'm giving queen ants within my simulation a lifespan of three to four simulation years and likewise worker ants will have a life span of three to four simulation months. The same article went on to talk about how ants sleep in short power naps lasting just over a minute two hundred and fifty times a day and the queen sleeps for longer periods of around six minutes nintey times a day, I also will reflect this in the context of my simulation. The concept of insects having different roles or jobs within a hive is explored in the work of Oster and Wilson (1978) (!cite) although I won't get into too much detail on this point I will outline different roles that ants within the simulation can take. Firstly there is a queen, in my simulation this will be dramatically simplified to one per colony, her job will to simply lay eggs to produce more workers.

\paragraph{}In the simulation the queen will be allowed to produce eggs at varying frequency depending on how much food and water is available to her and she will only produce offsring while there is room in the nest. Her main priority will be ensuring the colony doesn't run out of workers. Rhe 'worker' class of ants will then be subdivided into four seperate castes. Firstly a builder caste of ant will be created who's main priority is to build, maintain and repair the colony's nest. A builder ant's work will never be done as even when the nest is in excellent condition the ants will always strive to make the nest bigger so the queen can continue producing ants and for increased space for food storage. Builder caste ants will also be responsible for cleaning the nest. The second class of ants to exist within the simulation will be be the gatherers this castes' number one priority is to go out to known sources of food and water and carry it back to the colony's nest for storage. Slightly different from this class is the scout whose main purpose is to explore new territory and communicate to other ants where new sources of food and water are aswell as building materials useful for the builder caste of ant. Finally onr last caste will be outlined as the soldier ants whose main job is to protect the the nest and provide security, they will attempt to achieve this by patrolling the perimiter of the nest, the perimiter of the ant colony's territory as defined by the scout caste, and other important areas to the colony such as the food store and the queen.

\paragraph{}Within my artifact I'll have to represent each of these castes by varying values of attributes assigned to each ant. I will go into further detail about the attributes I will use to represent ants within my system in further detail later. But I will also have to represent communication between the ants, to keep communication between ants to some level of realism I will allow each ant within the simulation to produce pheremones. Pheremones will vary in strength  All ants will have the capability to release a pheremone indicating danger. When ants die they will release a very strong pheremone signaling general danger, but scout ants will be able to release strong pheremones indicating whether this is a danger all ants should flee from or if soldier ants should move in and either protect the nest. Situations will often occur where scout ants will call in the aid of soldier ants to attack a larger insect and once this attack is successful the target is now a food source. Scout ants will notify gatherer ants of this by again releasing a pheremone which indicates a food source has been discovered while returning to the nest. I will also allow builder ants to communicate with a similar system detecting scout pheremones indicating building resources or an area of damaged nest and react accordingly. Scout ants will be able to release most pheremones that all castes of ant are capable of producing while other castes range of pheremones will be more specialized. Details of of the planned pheremone structure will be shown in the table below (Table fig.~\ref{tab:foo})

\begin{table}
\begin{center}
	\begin{tabular}{lll}
		11 & 12 & 13 \\
		21 & 22 & 23 \\
		31 & 32 & 33 \\
	\end{tabular}
	\caption{Table of Foos}
	\label{tab:foo}
\end{center}
\end{table}

\section{Problem Area}
When the concept of the simulation is broken down it is immediately evident that a crutial element of the system to be produced is the ant. As mentioned before in a real world colony amount of ants in a colony can be very large, within my simulation however in order to produce a working final product an incremental approach will be taken to solve the problem loooking at first representing one ant then two ants and look at getting collision detection working between them before increasing the complexity of an ant and increasing the amount of ants in the system. Even though starting small will allow me to take a numerous amount of approaches to the problem The project will be aiming to take an approach that scales well, so that when we add more ants to the simulation the performance doesn't drop to a point where it is no longer accurate and realistic to a given definition of realism. This is important to note as when we look for a programming approach in the research stage of the paper it would be sensible choose one that inherently produces solutions capable of scaling well so that performance doesn't dip as the demand on the system is increased. With this in mind a notable observation that can be made of the problem is that it is rich in concurrency. Concurrency is a property attributted to a system which performs more than one ``possibly unrelated tasks at the same time.'' \cite{OSullivan2008} When holding this definition against my problem it is clear that my problem is a concurrent one.

\chapter{Research}
\section {Research Aims}
\paragraph{}Throughout the following section a breakdown of the topics that will be important to my project will be provided. Questions will then be derived from these topics in order to produce a set of aims that the research part of the paper will aim to address. As research will be an on going area of focus in the project not all the aims will be covered in this paper. When tackling a concurrent problem it would profitable to research what are the most popular and promising programming paradigms used to approach concurrent problems, taking this further to look at why these paradigms are more efficient at dealing with my problem. In the process of getting more specific, the focus will then be turned to what programming languages are used for this type of problem, again looking at why certain languages are perferred or avoided. Other important aspects of the project will also be further researched such as collision detection, the visual representation of the simulation and artificial intelligence concepts such as genetic algorithms and neural networks. However not all of the research will be included witin this paper as this will be an on going process. Most importantly a programming paradigm will be chosen to focus on ``the choice of programming paradigm can significantly influence the way one thinks about problems and expresses solutions'' to problems. \cite{Curriculum2008}

\paragraph{}To summarise the research aims of this project are as follows:
\begin{itemize}
	\item Identify a programming paradigm that is suitable for the problem.
	\item Compare languages that could be used in the implementation of the project.
	\item Review algorithms and Data Structures (represented in languages which may be used throughout this project) that tackle concurrency and other problems found in my project.
	\item Analyze other large scale simulations.
	\item Compare approaches to large scale collision detection.
	\item Look at possible ways to represent the information in the simulation visually.
\end{itemize}

\section{Programming Paradigms}
\paragraph{}``Over the last decades, several programming paradigms emerged and profiled. The most important ones are: imperative, object-oriented, functional, and logic paradigm.''\cite{Vujosevic2008} The next few paragraphs will look breifly at these four paradigms and go on to mention a few others which might be of interest considering the problem.

\subsection{The Imperative Paradigm}
The imperative programming paradigm is based on the Von Neumann architecture of computers, introduced in 1940â€™s. \cite{Vujosevic2008} This means the programming paradigm similar to the Von Neumann machine operate by performing one operation at a time, on a certain pieces of data retrieved from memory, in sequential order. According to Backus \cite{Backus1978} the man who cointed the term ``The von Neumann bottleneck'', ``there are several problems created by the word-at-a-time von Neumann style of programming, with its primitive use of loops, subscripts, and branching flow of control.''

\subsection{The Object Orientated Paradigm}
The Object orientated paradigm is as its name suggests center around objects. Its aim is to group similar data and functionality related to that data into objects. Blocking access to data within objects this way is commonly called encapsulation and is common in the Object orientated paradigm.
One advantages of programming in the Object Orientated paradigm %Disadvantages.. Reusable code. - link to Functional Programming.

\subsection{The Functional Paradigm}
Object orientated programming has been described as ``the antithesis of functiontional programming'' \cite{Taivalsaari1993} From the two programming paradigms above it can be deduced that a good programming language is modular. But John Hughes claims that it is not enough for a language to just support modularity, it needs to go a step further and make modular programming easy. To do this a programming language needs to provide flexible functionality in order to bring modules together. In an an article on Functional Programming Hughes writes ``Functional programming languages provide two new kinds of glue - higher-order functions and lazy evaluation.''\cite{Hughes1984} %High order functions and Lazy evaluation.

\subsection{The Logic Paradigm}
The logic programming paradigm - talk about inference and goals and pedicates. refer Prolog.
%Talk the relevancy of logic programming, and also the negatives.

\section{Why Parallelism}
Over the last few years hardware manufactures have shifted their focus from producing processors with faster clockspeeds. Traditionally a programmer would be able to produce a program which ran on the current processor, and to get it to run faster he would just wait for faster hardware to be released and run their code on that.  
%Tech companies have thrown a hail mary not going for increasing clockspeed anymore, Dave Patterson.

\paragraph{}
Parallelism is also viewed as a programming paradigm, it takes the concept that large problems can usually be broken down into smaller simpler problems. If these problems can be solved independant of one another then different processors can compute these problems at the same time. Parallelism can take place at different levels, there are levels of parallelism pertaining to hardware while others concepts in parallelism deal with the code. The two levels of parallelism that apply to code are the ones of particular concern here and are referred to Data and Task parallelism. Parallelism can also be see at the Instructional level. For instructions to be executed in parallel they need to be data independant. Take fo  Another area where parallelism can be identified is at the Bit level although this is more relevant to hardware development. There are also two approaches to programming in parallel implicit parallelism is all down to the language and the compiler. Programming in languages such as pH and NESL don't need specific functions or  takes Explicit parallelism,Concurrency) Explain differences

\paragraph{}
This project will focus on parallelism through functional langauges. It will persue this path because due to functional programming language features such as lazy evaluation it easier to write code that allows more computations to be evaluated seperately on either different threads, different cores and potentially different machines. In functional languages it is easier (...? Reuesable code, functional code scales better...?) However similar algorithms maybe generated in imperative, object orientated languages to show differences in behaviour and performance. 

\section{Choice and Use of Programming Language}

When looking for a language to develop this project in it was hard not to consider Haskell. Haskell

%Discuss Haskell and Erlang
%
1 What are functional programming languages?
Programming languages such as C/C++/Java/Python are called imperative programming languages because they consist of sequences of actions. The programmer quite explicitly tells the computer how to perform a task, step-by-step. Functional programming languages work differently. Rather than performing actions in a sequence, they evaluate expressions.

1.1 The level of abstraction
There are two areas that are fundamental to programming a computer - resource management and sequencing. Resource management (allocating registers and memory) has been the target of vast abstraction, most new languages (imperative as well as functional) have implemented garbage collection to remove resource management from the problem, and lets the programmer focus on the algorithm instead of the book-keeping task of allocating memory. Sequencing has also undergone some abstraction, although not nearly to the same extent. Imperative languages have done so by introducing new keywords and standard libraries. For example, most imperative languages have special syntax for constructing several slightly different loops, you no longer have to do all the tasks of managing these loops yourself. But imperative languages are based upon the notion of sequencing - they can never escape it completely. The only way to raise the level of abstraction in the sequencing area for an imperative language is to introduce more keywords or standard functions, thus cluttering up the language. This close relationship between imperative languages and the task of sequencing commands for the processor to execute means that imperative languages can never rise above the task of sequencing, and as such can never reach the same level of abstraction that functional programming languages can.

In Haskell, the sequencing task is removed. You only care what the program is to compute not how or when it is computed. This makes Haskell a more flexible and easy to use language. Haskell tends to be part of the solution for a problem, not a part of the problem itself.

1.2 Functions and side-effects in functional languages
Functions play an important role in functional programming languages. Functions are considered to be values just like integers or strings. A function can return another function, it can take a function as a parameter, and it can even be constructed by composing functions. This offers a stronger "glue" to combine the modules of your program. A function that evaluates some expression can take part of the computation as an argument for instance, thus making the function more modular. You could also have a function construct another function. For instance, you could define a function "differentiate" that will differentiate a given function numerically. So if you then have a function "f" you could define "f' = differentiate f", and use it like you would normally in a mathematical context. These types of functions are called higher order functions.

Here is a short Haskell example of a function numOf that counts the number of elements in a list that satisfy a certain property.

numOf p xs = length (filter p xs)
We will discuss Haskell syntax later, but what this line says is just "To get the result, filter the list xs by the test p and compute the length of the result". Now p is a function that takes an element and returns True or False determining whether the element passes or fails the test. So numOf is a higher order function, some of the functionality is passed to it as an argument. Notice that filter is also a higher order function, it takes the "test function" as an argument. Let's play with this function and define some more specialized functions from it.

numOfEven xs = numOf even xs
Here we define the function numOfEven which counts the number of even elements in a list. Note that we do not need to explicitly declare xs as a parameter. We could just as well write numOfEven = numOf even. A very clear definition indeed. But we'll explicitly type out the parameters for now.

Let's define a function which counts the number of elements that are greater or equal to 5 :

numOfGE5 xs = numOf (>=5) xs
Here the test function is just ">=5" which is passed to numOf to give us the functionality we need.

Hopefully you should now see that the modularity of functional programming allows us to define a generic functions where some of the functionality is passed as an argument, which we can later use to define shorthands for any specialized functions. This small example is somewhat trivial, it wouldn't be too hard to re-write the function definition for all the functions above, but for more complex functions this comes in handy. You can, for instance, write only one function for traversing an auto-balancing binary tree and have it take some of the functionality as a parameter (for instance the comparison function). This would allow you to traverse the tree for any data type by simply providing the relevant comparison function for your needs. Thus you can expend some effort in making sure the general function is correct, and then all the specialized functions will also be correct. Not to mention you wouldn't have to copy and paste code all over your project. This concept is possible in some imperative languages as well. In some object oriented languages you often have to provide a "Comparator object" for trees and other standard data structures. The difference is that the Haskell way is a lot more intuitive and elegant (creating a separate type just for comparing two other types and then passing an object of this type is hardly an elegant way of doing it), so it's more likely to be used frequently (and not just in the standard libraries).

A central concept in functional languages is that the result of a function is determined by its input, and only by its input. There are no side-effects! This extends to variables as well - variables in Haskell do not vary. This may sound strange if you're used to imperative programming (where most of the code consists of changing the "contents" of a variable), but it's really quite natural. A variable in Haskell is a name that is bound to some value, rather than an abstraction of some low-level concept of a memory cell like in imperative languages. When variables are thought of as short-hands for values (just like they are in mathematics), it makes perfect sense that variable updates are not allowed. You wouldn't expect "4 = 5" to be a valid assignment in any language, so it's really quite strange that "x = 4; x = 5" is. This is often hard to grasp for programmers who are very used to imperative languages, but it isn't as strange as it first seems. So when you start thinking things like "This is too weird, I'm going back to C++!", try to force yourself to continue learning Haskell - you'll be glad you did.

Removing side-effects from the equation allows expressions to be evaluated in any order. A function will always return the same result if passed the same input - no exceptions. This determinism removes a whole class of bugs found in imperative programs. In fact, you could even argue that most bugs in large systems can be traced back to side-effects - if not directly caused by them, then caused by a flawed design that relies on side-effects. This means that functional programs tend to have far fewer bugs than imperative ones.

1.3 Conclusion
Because functional languages are more intuitive and offer more and easier ways to get the job done, functional programs tend to be shorter (usually between 2 to 10 times shorter). The semantics are most often a lot closer to the problem than an imperative version, which makes it easier to verify that a function is correct. Furthermore Haskell doesn't allow side-effects, which leads to fewer bugs. Thus Haskell programs are easier to write, more robust, and easier to maintain.

2 What can Haskell offer the programmer?
Haskell is a modern general purpose language developed to incorporate the collective wisdom of the functional programming community into one elegant, powerful and general language.

2.1 Purity
Unlike some other functional programming languages Haskell is pure. It doesn't allow any side-effects. This is probably the most important feature of Haskell. We've already briefly discussed the benefits of pure, side-effect free, programming - and there's not much more we can say about that. You'll need to experience it yourself.

2.2 Laziness
Another feature of Haskell is that it is lazy (technically speaking, it's "non-strict"). This means that nothing is evaluated until it has to be evaluated. You could, for instance, define an infinite list of primes without ending up in infinite recursion. Only the elements of this list that are actually used will be computed. This allows for some very elegant solutions to many problems. A typical pattern of solving a problem would be to define a list of all possible solutions and then filtering away the illegal ones. The remaining list will then only contain legal solutions. Lazy evaluation makes this operation very clean. If you only need one solution you can simply extract the first element of the resulting list - lazy evaluation will make sure that nothing is needlessly computed.

2.3 Strong typing
Furthermore Haskell is strongly typed, this means just what it sounds like. It's impossible to inadvertently convert a Double to an Int, or follow a null pointer. This also leads to fewer bugs. It might be a pain in the neck in the rare cases where you need to convert an Int to a Double explicitly before performing some operation, but in practice this doesn't happen often enough to become a nuisance. In fact, forcing each conversion to be explicit often helps to highlight problem code. In other languages where these conversions are invisible, problems often arise when the compiler treats a double like an integer or, even worse, an integer like a pointer.

Unlike other strongly typed languages types in Haskell are automatically inferred. This means that you very rarely have to declare the types of your functions, except as a means of code documentation. Haskell will look at how you use the variables and figure out from there what type the variable should be - then it will all be type-checked to ensure there are no type-mismatches. Python has the notion of "duck typing", meaning "If it walks and talks like a duck, it's a duck!". You could argue that Haskell has a much better form of duck typing. If a value walks and talks like a duck, then it will be considered a duck through type inference, but unlike Python the compiler will also catch errors if later on it tries to bray like a donkey! So you get the benefits of strong typing (bugs are caught at compile-time, rather than run-time) without the hassle that comes with it in other languages. Furthermore Haskell will always infer the most general type on a variable. So if you write, say, a sorting function without a type declaration, Haskell will make sure the function will work for all values that can be sorted.

Compare how you would do this in certain object oriented languages. To gain polymorphism you would have to use some base class, and then declare your variables as instances of subclasses to this base class. It all amounts to tons of extra work and ridiculously complex declarations just to proclaim the existence of a variable. Furthermore you would have to perform tons of type conversions via explicit casts - definitely not a particularly elegant solution. If you want to write a polymorphic function in these object oriented languages you would probably declare the parameters as an object of a global base class (like "Object" in Java), which essentially allows the programmer to send anything into the function, even objects which can't logically be passed to the function. The end result is that most functions you write in these languages are not general, they only work on a single data type. You're also moving the error checking from compile-time to run-time. In large systems where some of the functionality is rarely used, these bugs might never be caught until they cause a fatal crash at the worst possible time.

Haskell provides an elegant, concise and safe way to write your programs. Programs will not crash unexpectedly, nor produce strangely garbled output.

2.4 Elegance
Another property of Haskell that is very important to the programmer, even though it doesn't mean as much in terms of stability or performance, is the elegance of Haskell. To put it simply: stuff just works like you'd expect it to.

To highlight the elegance of Haskell we shall now take a look at a small example. We choose QuickSort-inspired filtering sort because it's a simple algorithm that is actually useful. We will look at two versions - one written in C++, an imperative language, and one written in Haskell. Both versions use only the functionality available to the programmer without importing any extra modules (otherwise we could just call "sort" in each language's standard library and be done with it!). Thus, we use the standard sequence primitives of each language (a "list" in Haskell and an "array" in C++). Both versions must also be polymorphic (which is done "automatically" in Haskell, and with templates in C++). Both versions must use the same recursive algorithm.

Please note that this is not intended as a definite comparison between the two languages. It's intended to show the elegance of Haskell, the C++ version is only included for comparison (and would be coded quite differently if you used the standard QuickSort algorithm or the Standard Template Libraries (STL), for example).

template <typename T>
void qsort (T *result, T *list, int n)
{
    if (n == 0) return;
    T *smallerList, *largerList;
    smallerList = new T[n];
    largerList = new T[n];      
    T pivot = list[0];
    int numSmaller=0, numLarger=0;      
    for (int i = 1; i < n; i++)
        if (list[i] < pivot)
            smallerList[numSmaller++] = list[i];
        else 
            largerList[numLarger++] = list[i];
    
    qsort(smallerList,smallerList,numSmaller); 
    qsort(largerList,largerList,numLarger);
    
    int pos = 0;        
    for ( int i = 0; i < numSmaller; i++)
        result[pos++] = smallerList[i];
    
    result[pos++] = pivot;
    
    for ( int i = 0; i < numLarger; i++)
        result[pos++] = largerList[i];
    
    delete [] smallerList;
    delete [] largerList;
};
We will not explain this code further, just note how complex and difficult it is to understand at a glance, largely due to the programmer having to deal with low-level details which have nothing to do with the task at hand. Now, let's take a look at a Haskell version of FilterSort, which might look a something like this.

qsort :: (Ord a) => [a] -> [a]
 qsort []     = []
 qsort (x:xs) = qsort less ++ [x] ++ qsort more
     where less = filter (<x)  xs
           more = filter (>=x) xs
(This implementation has very poor runtime and space complexity, but that can be improved, at the expense of some of the elegance.)

Let's dissect this code in detail, since it uses quite a lot of Haskell syntax that you might not be familiar with.

The first line is a type signature. It declares "qsort" to be function that takes a list "[a]" as input and returns ("->") another list "[a]". "a" is a type variable (vaguely similar to a C++ template declaration), and "(Ord a)" is a constraint that means that only types that have an ordering are allowed. This function is a generic ("template") function, that can sort any list of pairwise-comparable objects. The phrase "(Ord a) => [a] -> [a]" means "if the type 'a' is ordered, than a list of 'a' can be passed in, and another list of 'a' will come out."

The function is called qsort and takes a list as a parameter. We define a function in Haskell like so: funcname a b c = expr, where funcname is the name of the function, a, b, and, c are the parameters and expr is the expression to be evaluated (most often using the parameters). Functions are called by simply putting the function name first and then the parameter(s). Haskell doesn't use parenthesis for function application. Functions simply bind more tightly than anything else, so "f 5 * 2", for instance, would apply f to 5 and then multiply by 2, if we wanted the multiplication to occur before the function application then we would use parenthesis like so "f (5*2)".

Let's get back to FilterSort. First we see that we have two definitions of the functions. This is called pattern matching and we can briefly say that it will test the argument passed to the function top-to-bottom and use the first one that matches. The first definition matches against [] which in Haskell is the empty list (a list of 1,2 and 3 is [1,2,3] so it makes sense that an empty list is just two brackets). So when we try to sort an empty list, the result will be an empty list. Sounds reasonable enough, doesn't it? The second definition pattern matches against a list with at least one element. It does this by using (x:xs) for its argument. The "cons" operator is (:) and it simply puts an element in front of a list, so that 0 : [1,2,3] returns [0,1,2,3]. Pattern matching against (x:xs) is a match against the list with the head x and the tail xs (which may or may not be the empty list). In other words, (x:xs) is a list of at least one element. So since we will need to use the head of the list later, we can actually extract this very elegantly by using pattern matching. You can think of it as naming the contents of the list. This can be done on any data construct, not just a list. It is possible to pattern match against an arbitrary variable name and then use the head function on that to retrieve the head of the list. Now if we have a non empty list, the sorted list is produced by sorting all elements that are smaller than x and putting that in front of x, then we sort all elements larger than x and put those at the end. We do this by using the list concatenation operator ++. Notice that x is not a list so the ++ operator won't work on it alone, which is why we make it a singleton-list by putting it inside brackets. So the function reads "To sort the list, sandwich the head between the sorted list of all elements smaller than the head, and the sorted list of all elements larger than the head". Which could very well be the original algorithm description. This is very common in Haskell. A function definition usually resembles the informal description of the function very closely. This is why I say that Haskell has a smaller semantic gap than other languages.

But wait, we're not done yet! How is the list less and more computed? Well, remember that we don't care about sequencing in Haskell, so we've defined them below the function using the where notation (which allows any definitions to use the parameters of the function to which they belong). We use the standard prelude function filter, I won't elaborate too much on this now, but the line less = filter (<x) xs will use filter (<x) xs to filter the list xs. You can see that we actually pass the function which will be used to filter the list to filter, an example of higher order functions. The function (<x) should be read out "the function 'less than x'" and will return True if an element passed to it is less than x (notice how easy it was to construct a function on the fly, we put the expression "<x", "less than x", in parenthesis and sent it off to the function - functions really are just another value!). All elements that pass the test are output from the filter function and put inside less. In a same way (>=x) is used to filter the list for all elements larger than or equal to x.

Now that you've had the syntax explained to you, read the function definition again. Notice how little time it takes to get an understanding about what the function does. The function definitions in Haskell explain what it computes, not how.

If you've already forgotten the syntax outlined above, don't worry! We'll go through it more thoroughly and at a slower pace in the tutorials. The important thing to get from this code example is that Haskell code is elegant and intuitive.

2.5 Haskell and bugs
We have several times stated that various features of Haskell help fight the occurrence of bugs. Let's recap these.

Haskell programs have fewer bugs because Haskell is:

Pure. There are no side effects.
Strongly typed. There can be no dubious use of types. And No Core Dumps!
Concise. Programs are shorter which make it easier to look at a function and "take it all in" at once, convincing yourself that it's correct.
High level. Haskell programs most often reads out almost exactly like the algorithm description. Which makes it easier to verify that the function does what the algorithm states. By coding at a higher level of abstraction, leaving the details to the compiler, there is less room for bugs to sneak in.
Memory managed. There's no worrying about dangling pointers, the Garbage Collector takes care of all that. The programmer can worry about implementing the algorithm, not book-keeping of the memory.
Modular. Haskell offers stronger and more "glue" to compose your program from already developed modules. Thus Haskell programs can be more modular. Often used modular functions can thus be proven correct by induction. Combining two functions that are proven to be correct, will also give the correct result (assuming the combination is correct).
Furthermore most people agree that you just think differently when solving problems in a functional language. You subdivide the problem into smaller and smaller functions and then you write these small (and "almost-guaranteed-to-be-correct") functions, which are composed in various ways to the final result. There just isn't any room for bugs!


3 Haskell vs OOP
The great benefit of Object Oriented Programming is rarely that you group your data with the functions that act upon it together into an object - it's that it allows for great data encapsulation (separating the interface from implementation) and polymorphism (letting a whole set of data types behave the same way). However:

Data encapsulation and polymorphism are not exclusive to OOP!

Haskell has tools for abstracting data. We can't really get into it without first going through the module system and how abstract data types (ADT) work in Haskell, something which is well beyond the scope of this essay. We will therefore settle for a short description of how ADTs and polymorphism works in Haskell.

Data encapsulation is done in Haskell by declaring each data type in a separate module, from this module you only export the interface. Internally there might be a host of functions that touch the actual data, but the interface is all that's visible from outside of the module. Note that the data type and the functions that act upon the data type are not grouped into an "object", but they are (typically) grouped into the same module, so you can choose to only export certain functions (and not the constructors for the data type) thus making these functions the only way to manipulate the data type - "hiding" the implementation from the interface.

Polymorphism is done by using something called type classes. Now, if you come from a C++ or Java background you might associate classes with something resembling a template for how to construct an object, but that's not what they mean in Haskell. A type class in Haskell is really just what it sounds like. It's a set of rules for determining whether a type is an instance of that class. So Haskell separates the class instantiation and the construction of the data type. You might declare a type "Porsche", to be an instance of the "Car" type class, say. All functions that can be applied onto any other member of the Car type class can then be applied to a Porsche as well. A class that's included with Haskell is the Show type class, for which a type can be instantiated by providing a show function, which converts the data type to a String. Consequently almost all types in Haskell can be printed onto the screen by applying show on them to convert them to a String, and then using the relevant IO action (more on IO in the tutorials). Note how similar this is to the the object notion in OOP when it comes to the polymorphism aspect. The Haskell system is a more intuitive system for handling polymorphism. You won't have to worry about inheriting in the correct hierarchical order or to make sure that the inheritance is even sensible. A class is just a class, and types that are instances of this class really doesn't have to share some parent-child inheritance relationship. If your data type fulfills the requirements of a class, then it can be instantiated in that class. Simple, isn't it? Remember the QuickSort example? Remember that I said it was polymorphic? The secret behind the polymorphism in qsort is that it is defined to work on any type in the Ord type class (for "Ordered"). Ord has a set of functions defined, among them is "<" and ">=" which are sufficient for our needs because we only need to know whether an element is smaller than x or not. So if we were to define the Ord functions for our Porsche type (it's sufficient to implement, say, <= and ==, Haskell will figure out the rest from those) in an instantiation of the Ord type class, we could then use qsort to sort lists of Porsches (even though sorting Porsches might not make sense). Note that we never say anything about which classes the elements of the list must be defined for, Haskell will infer this automatically from just looking at which functions we have used (in the qsort example, only "<" and ">=" are relevant).

So to summarize: Haskell does include mechanisms for data encapsulation that match or surpass those of OOP languages. The only thing Haskell does not provide is a way to group functions and data together into a single "object" (aside from creating a data type which includes a function - remember, functions are data!). This is, however, a very minor problem. To apply a function to an object you would write "func obj a b c" instead of something like "obj.func a b c".


4 Modularity
A central concept in computing is modularity. A popular analogy is this: say you wanted to construct a wooden chair. If you construct the parts of it separately, and then glue them together, the task is solved easily. But if you were to carve the whole thing out of a solid piece of wood, it would prove to be quite a bit harder. John Hughes had this to say on the topic in his paper Why Functional Programming Matters

"Languages which aim to improve productivity must support modular programming well. But new scope rules and mechanisms for separate compilation are not enough - modularity means more than modules. Our ability to decompose a problem into parts depends directly on our ability to glue solutions together. To assist modular programming, a language must provide good glue.

Functional programming languages provide two new kinds of glue - higher-order functions and lazy evaluation."


5 The speed of Haskell
Let me first state clearly that the following only applies to the general case in which speed isn't absolutely critical, where you can accept a few percent longer execution time for the benefit of reducing development time greatly. There are cases when speed is the primary concern, and then the following section will not apply to the same extent.

Now, some C++ programmers might claim that the C++ version of QuickSort above is probably a bit faster than the Haskell version. And this might be true. For most applications, though, the difference in speed is so small that it's utterly irrelevant. For instance, take a look at the Computer Language Benchmarks Game, where Haskell compares favorably to most of the so called "fast" languages. Now, these benchmarks don't prove all that much about real-world performance, but they do show that Haskell isn't as slow as some people think. At the time of writing it's in 4th position, only slightly behind C and C++.

Almost all programs in use today have a fairly even spread of processing time among its functions. The most notable exceptions are applications like MPEG encoders, and artificial benchmarks, which spend a large portion of their execution time within a small portion of the code. If you really need speed at all costs, consider using C instead of Haskell.

There's an old rule in computer programming called the "80/20 rule". It states that 80\% of the time is spent in 20\% of the code. The consequence of this is that any given function in your system will likely be of minimal importance when it comes to optimizations for speed. There may be only a handful of functions important enough to optimize. These important functions could be written in C (using the excellent foreign function interface in Haskell). The role of C could, and probably will, take over the role of assembler programming - you use it for the really time-critical bits of your system, but not for the whole system itself.

We should continue to move to higher levels of abstraction, just like we've done before. We should trade application speed for increased productivity, stability and maintainability. Programmer time is almost always more expensive than CPU time. We aren't writing applications in assembler anymore for the same reason we shouldn't be writing applications in C anymore.

Finally remember that algorithmic optimization can give much better results than code optimization. For theoretical examples when factors such as development times and stability doesn't matter, then sure, C is often faster than Haskell. But in the real world development times do matter, this isn't the case. If you can develop your Haskell application in one tenth the time it would take to develop it in C (from experience, this is not at all uncommon) you will have lots of time to profile and implement new algorithims. So in the "real world" where we don't have infinite amounts of time to program our applications, Haskell programs can often be much faster than C programs.

6 Epilogue
So if Haskell is so great, how come it isn't "mainstream"? Well, one reason is that the operating system is probably written in C or some other imperative language, so if your application mainly interacts with the internals of the OS, you may have an easier time using imperative languages. Another reason for the lack of Haskell, and other functional languages, in mainstream use is that programming languages are rarely thought of as tools (even though they are). To most people their favorite programming language is much more like religion - it just seems unlikely that any other language exists that can get the job done better and faster.

There is an essay by Paul Graham called Beating the Averages describing his experience using Common Lisp, another functional language, for an upstart company. In it he uses an analogy which he calls "The Blub Paradox".

It goes a little something like this: If a programmer's favorite language is Blub, which is positioned somewhere in the middle of the "power spectrum", he can most often only identify languages that are lower down in the spectrum. He can look at COBOL and say "How can anyone get anything done in that language, it doesn't have feature x", x being a feature in Blub.

However, this Blub programmer has a harder time looking the other way in the spectrum. If he examines languages that are higher up in the power spectrum, they will just seem "weird" because the Blub programmer is "thinking in Blub" and can not possibly see the uses for various features of more powerful languages. It goes without saying that this inductively leads to the conclusion that to be able to compare all languages you'll need to position yourself at the top of the power spectrum. It is my belief that functional languages, almost by definition, are closer to the top of the power spectrum than imperative ones. So languages can actually limit a programmers frame of thought. If all you've ever programmed is Blub, you may not see the limitations of Blub - you may only do that by switching to another level which is more powerful.

One of the reasons the mainstream doesn't use Haskell is because people feel that "their" language does "everything they need". And of course it does, because they are thinking in Blub! Languages aren't just technology, it's a way of thinking. And if you're not thinking in Haskell, it is very hard to see the use of Haskell - even if Haskell would allow you to write better applications in a shorter amount of time!

Hopefully this article has helped you break out of the Blub paradox. Even though you may not yet "think in Haskell", it is my hope that you are at least aware of any limitations in your frame of thought imposed by your current "favorite" language, and that you now have more motivation to expand it by learning something new.

If you are committed to learn a functional language, to get a better view of the power spectrum, then Haskell is an excellent candidate.
% REFERENCE HASKELL WIKI

%Actors why they're interesting (distrubuted programming)
%Mention the vast array of Haskell libraries used to support the language.

\section {Brief History of Parallelism}
People as early as, Flynn were thinking about the power that could be harnessed from being able to compute things in parallel.
Flynn's Taxonomy is a classification of the different methohd of processing data. Flynn defined four methods these were, SISD- Single Instruction Single Data, SIMD- Single Instruction Multi data, MISD- Multi Instruction Multi Data and MIMD- Multi Instruction Multi Data. Even today it is still good approach to look at software an
Models Kuck and its

\section {What problems can be parallelized?}
In theory any problem which has the ability to perform more than one calculation at the same time has the potential to be parallelized. However some calculations in an algorithm may have Data dependencies, 
Dave Patterson. Computer Architecture.

\subsection{The Limits of Parralelization}
It has been proved by Gene Amdahl that increasing the amount of processors working on a program only increases speed for so long. Amdahl's law shows that the speedup of a program is limited by the sequential portion of the program. This is clear because sequential processes need to be performed in sequence so by definition they are prevented from benifiting from multiple processors. In 1967 at the AFIPS Spring joint Computer Conference, while talking about the overhead of ``data management housekeeping'' which can be found in any computer program, Amdahl reportedly said that ``The nature of this overhead appears to be sequential so that it is unlikely to be amenable to parallel processing techniques. Overhead alone would then place an upper limit on throughput.''\cite{Amdahl1967} This upper limit has been deduced from his conference talk and given rise to the widely known formula below.
% -Amdahl's law latex forumula
The formula calculates the potential speedup that could be brought to a program by adding more processors work on the parallel portion of a problem. The sequential overhead generated in arranging the parallel computation that Amdahl mentions is represented by (1-P) and therefore P representing the parallel section of the program makes up the whole. The forumula then takes the sequential portion and adds it to the parallel portion of the program over N which represents the number of processors this symbolizes the division of the parallel portion of the program between the processors. The upper limit that Amdahl referrs to is clear as you increase the number of processors N, if you had infinity processors working on the parallel part the speed up would simply be 1 over (1-P).
\cite{Amdahl1967}

\paragraph{}
There have however been criticisms of Amdahl's law
Gustafson's law, Cost efficiency, Karp-Flatt metric

\paragraph{}
Non-blocking algorithm.
slowdown and speedup

\section{Algorithms and Data Structures}
The vast majority of of programs or as Skiena puts it any reasonable computer program has a procedure to to accomplish a specific task. These procedures are called algorithms and play an important role throughout any programming project.\cite{Skiena2010} As the project progressing a close look will be taken at the different known algorithms which tackle the problems that arise and particular attenton will be be taken in order to use algorithms that scale well with the quantity of data passing through them. Also I will 
\section {Space Partitioning}
Binary Space Partitioning and KD Trees
\section{Collision Detection}
There are several algorithms for collision detection as it is a well researched field. Collision detection is not only found in games but in many areas such as physics and other scientific simulations as well as robotics.
Posteri Vs Priori
?Hybrid parallel continuous collision detection
\section{Artificial Intelligence} 

\section{Graphical Style}


\chapter{Planning}


\section{Software Development Model}
This project will take an iterative and modular development approach as it works well with the nature of the program. It is a useful coding practise to develop small modules which are capable of functioning independantly from the code as these modules can then be reused, in developing other programs. There is also another advantage to be drawn from modular development, each module would be able to be tested sperately which would in turn ensure less bugs when assembling the final product.

\section{Important Tasks}
The following is a break down of the project into smaller iterations this can then be used alongside the various project deadlines I have, to produce a Gantt chart of how I plan and how the workflow will go.
\begin{itemize}
	\item Representing an Ant
	\item Representing the World
	\item Representing the Pheremone Levels
	\item Individual Ant Intelligence
	\item Collision Detection
	\item Parallelize Algorithms further
	\item Distributing Problem
\end{itemize}

\section{A Schedule of Activities}
The produced Gantt chart attached to this report visualizes the following information. Throughout the following paragraphs I will describe my plan for the project and in the form of a schedule. 

\paragraph{October-November}
Throughout the course of October and November the project will focus mainly on gathering resources  and research. Much of research will be conducted in the functional languages Haskell and Erlang and small experiments will be made to demonstrate any concepts that have been learnt. Further research will be carried out in areas of the project that will pose potential problems that need to be dealt with later on in the implementation stage of the project. Even at this point in the project the implementation process will begin. As development will be iterative small aims will be set such as representing a single ant and then these aims will be expanded upon.
\paragraph{December-Janurary}
In the months of December and Janurary the code produced in the first section will be expanded upon to produce a working base system. At the end of this this period a simulation of the ant colony should be able to be run. The simulation should include the all the basic functionality of the final system. Ants will be able to move and collide. Each ant will respond to its surrounding by referencing its own basic behaviour tree.
\paragraph{February-March}
In Feburary and March I will focus on improving the algorithms I have used within the system. Making them more efficient and try and get more of them working in parallel. At the end of this period the project's delieverable, the simulation, will be in a state that is presentable with as few bugs as possible. Towards the end of this period if things are on schedule the project's focus will shift from improving the base simulation to adding extentions and making it more usable.
\paragraph{April}
During the final few weeks of the project I will be fixing any bugs in the simulation and finishing up any extentions that may be added to the project. During this time the projects focus will be documentation and producing an informative and evaluative technical report.

\section{Risk Analysis}
Throughout this project there are several things that could go wrong, such as  loss of work, hardware failure and the inability to surpass certain bugs I encounter throughout the project. To minimize the chances of loss of work I will use git source control tools to not only backup my project but keep a track of changes and monitor its evolution. In the event of hardware failiure all the software I require for the project is freely available and I would be able to set up for work either in the University or on a replacement system with minimal effort. There is also a small chance that I may encounter bugs which are impossible to over come this is a risk that can only be managed through planning and research. Should I encounter difficulties there are a huge range of resources for functional programming and a Brighton functional programming user group, this would provide a great opportunity to increase my knowledge but should I not be able to find a solution the only course of action would be to alter the future plans of the project.
%(!TODO) A risk analysis of potential problems.

\section{Ethics}
Before embarking on this project it was essential to look into any ethical issues that may arise throughout the course of the project or as a result of using the project's delieverable. 
So far none have no ethical issues have been encountered however ethics  will be kept in mind throughout the course of the project and any issues will be documented in the project log during the project and the technical report at the end of the project. 



%\begin{figure}
%  \begin{center}
%   \includegraphics{images/foo}
%   \caption{My caption}
%   \label{fig:foo}
%  \end{center}
%\end{figure}

%\begin{lstlisting}
%\input{src/fib.hs}
%\end{lstlisting}

\bibliographystyle{plain}
\bibliography{Biblio}
%(!TODO)A report on your background research with an annotated bibliography.
% How to do this in LATEX?!
\end{document}























