Pre amble
Swarm Intelligence: From Natural to Artificial Systems
Eric Bonabeau, Marco Dorigo and Guy Theraulaz,New York, NY: Oxford University Press, Santa Fe Institute Studies in the Sciences of Complexity (1999)
Paper: ISBN 0-19-513159-2

First Interactive Ray tracing Engine launched by Nvidia Optix
http://www.nvidia.com/object/io_1249366628071.html

REAL TIME RAYTRACING -mention of K-D trees
Accelerated entry point search algorithm for real-time ray-tracing
Colin Fowler, Steven Collins, Michael Manzke
April 2009		
SCCG '09: Proceedings of the 2009 Spring Conference on Computer Graphics


Ant facts
http://news.bbc.co.uk/earth/hi/earth_news/newsid_8100000/8100876.stm
http://insected.arizona.edu/antrear.htm
Different roles
Oster GF, Wilson EO (1978). Caste and ecology in the social insects. Princeton University Press, Princeton. pp. 21–22. ISBN 0-691-02361-1.

Parralell the future
http://parlab.eecs.berkeley.edu/publication/137
Title	The Landscape of Parallel Computing Research: A View from Berkeley :: Publication Type	Journal Article
Year of Publication	2006 Authors	Asanovic, K., Bodik R., Catanzaro B., Gebis J. J., Husbands P., Keutzer K., Patterson D., Plishker W. L., Shalf J., Williams S., & Yelick K. A.
*****Parallelism is an energy-efficient way to achieve performance [Chandrakasan et al 1992].

The essence of the problem facing the parallel programmer is that, in addition to specifying what value the program should compute, explicitly parallel programs must also specify how the machine should organise the computation. Simon Peyton Jones in the Journal of Function Programming  link_http://www.macs.hw.ac.uk/~dsg/gph/papers/abstracts/strategies.html

Edgar Dijkstra's !!!!!!!!!!!!
4.  Dijkstra,  E.W. Solution of  a problem in concurrent  program- 
ming control.  C o m m .   A C M   8, 9 (Sept. 1965), 569. 
5.  Dijkstra, E.W. The structure of THE multiprogramming system. 
C o m m .   A C M   11,  5  (May 1968), 341-346. 




On the Karp–Flatt metric -  an indication of the extent to which a particular computer code is parallelized. It was proposed by Alan H. Karp and Horace P. Flatt in 1990.
Karp, Alan H.; Flatt, Horace P. (1990). "Measuring Parallel Processor Performance". Communication of the ACM 33 (5): 539–543.
Quinn, Michael J. (2004). Parallel Programming in C with MPI and OpenMP. Boston: McGraw-Hill.


On Cost effectiveness
Advanced Computer Architectures: A Design Space Approach, D. Sima, T. Fountain and P. Kacsuk, Addison-Wesley, 1997.



Amdahls law
The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program. For example, if 95% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 20×, no matter how many processors are used.
Amdahl, Gene (1967). "Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities" (PDF). AFIPS Conference Proceedings (30): 483–485.
Rodgers, David P. (June 1985). "Improvements in multiprocessor system design". ACM SIGARCH Computer Architecture News archive (New York, NY, USA: ACM) 13 (3): 225–231. doi:10.1145/327070.327215. ISSN 0163-5964.

COMPARE with Hilis and Gustafson

Danny Hillis (in the 1998 book Pattern on the Stone) has criticized Amdahl's Law as being unnecessarily pessimistic in its assumption that the sequential portion of a program as being 5% (or 50%) of the execution time. In many applications, particularly with very large data sets (such as Google calculating PageRank, or scientific programs doing FEA), the amount of sequential code is closer to 0%, as essentially every data element can be processed independently.

John L. Gustafson pointed out in 1988 what is now known as Gustafson's Law: people typically are not interested in solving a fixed problem in the shortest possible period of time, as Amdahl's Law describes, but rather in solving the largest possible problem (e.g. the most accurate possible approximation) in a fixed "reasonable" amount of time. If the non-parallelizable portion of the problem is fixed, or grows very slowly with problem size (e.g. O(log n)), then additional processors can increase the possible problem size without limit.
^ Reevaluating Amdahl's Law, John L. Gustafson, Communications of the ACM 31(5), 1988. pp. 532-533. 


Bit level parallelism
Parallelism in hardware - increasing the word size.
David E. Culler, Jaswinder Pal Singh, Anoop Gupta. Parallel Computer Architecture -

Instructional level parallelism
a= w+x
b= y-z
c= a/b
instructions a and be can be computed at either time because c can't be computed without a and being computed first it can't be calculated until both are finished. closely linked with data dependancy(?)
http://dl.acm.org/citation.cfm?id=1074489

Data parallelism level
is a form of parallelization of computing across multiple processors in parallel computing environments. Data parallelism focuses on distributing the data across different parallel computing nodes. (Generally SIMD?)
Hillis, W. Daniel and Steele, Guy L., Data Parallel Algorithms Communications of the ACM December 1986
Blelloch, Guy E, Vector Models for Data-Parallel Computing MIT Press 1990. ISBN 0-262-02313-X

CONTRASTS WITH

Task Parallelism
 is a form of parallelization of computer code across multiple processors in parallel computing environments. Task parallelism focuses on distributing execution processes (threads) across different parallel computing nodes. 
Quinn Michael J, Parallel Programming in C with MPI and OpenMP McGraw-Hill Inc. 2004. ISBN 0-07-058201-7
D. Kevin Cameron coined terms "data static" and "code static".

***http://en.wikipedia.org/wiki/Algorithmic_skeleton***
TONNES OF SOURCES about functional programming patterns!!!

***http://en.wikipedia.org/wiki/Scalable_Parallelism***
Will my project be able to make use of multiple processors to solve its problem?




Superthreading- use of timeslicing to fake SMT... ;/ bad times.
http://arstechnica.com/old/content/2002/10/hyperthreading.ars/3
While this approach enables better use of the processor's resources, further improvements to resource utilization can be realized through SMT, which allows the execution of instructions from multiple threads at the same time. Consider a two-way super-threaded processor with four functional units. If thread one issues three instructions, one functional unit remains unused. In a SMT processor, it is possible for thread two to issue an instruction to the remaining unit, attaining full utilization of processor resources. HUH?!

Hyper Threading
Intels early attempts at going parallel...

SMT!!! simultaneous yo!
is a technique for improving the overall efficiency of superscalar CPUs with hardware multithreading. SMT permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures.
--:/
Simultaneous multithreading cannot improve performance if any of the shared resources are limiting bottlenecks for the performance. In fact, some applications run slower when simultaneous multithreading is enabled. Critics argue that it is a considerable burden to put on software developers that they have to test whether simultaneous multithreading is good or bad for their application in various situations and insert extra logic to turn it off if it decreases performance. Current operating systems lack convenient API calls for this purpose and for preventing processes with different priority from taking resources from each other.[1]
There is also a security concern with certain simultaneous multithreading implementations. Intel's hyperthreading implementation has a vulnerability through which it is possible for one application to steal a cryptographic key from another application running in the same processor by monitoring its cache use.[2]
There is also a disadvantage if you want to use a PC for 100% with maximum performance, solving a single problem.
http://agner.org/optimize/blog/read.php?i=6&v=t
http://www.daemonology.net/hyperthreading-considered-harmful/

LE Shar and ES Davidson, "A Multiminiprocessor System Implemented through Pipelining", Computer Feb 1974
D.M. Tullsen, S.J. Eggers, and H.M. Levy, "Simultaneous Multithreading: Maximizing On-Chip Parallelism," In 22nd Annual International Symposium on Computer Architecture, June, 1995
D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm, "Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor," In 23rd Annual International Symposium on Computer Architecture, May, 1996


The memory wall problem
http://www.csl.cornell.edu/~sam/papers/cf04.pdf

Aidan RECCOMM!
HopCRoFT Mutwani - Introduction to automata theory! - BIG O-Notation...
Mutwani----Google--- Sergey Brin Larry Page.

